{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wealth of Cities\n",
    "## Predicting the Wealth of a City from Satellite Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurate measurements of the economic characteristics of populations critically influence research and policy. Such measurements can shape decisions by governments and how to allocate resources and provide infrastructure to improve human livelihoods in a wide-range of situations. Although economic data is readily available for some developing nations, many regions of the modern world remain unexposed to the benefits of economic analysis where regions lack key measures of economic development and efficiency. Regions such as parts of Africa conduct little to no economic surveys or other means of collecting data on their financial situations. We attempt to address this problem by using publicly available satellite imagery to predict the wealth of a city (or, more generally, a geographic region) based on fundamental features identified in these images and running them through a convolutional neural network. Not only would this method be applicable to regions that lack economic data, but could also be applied to cities with a wealth of economic information on a macro level but a dearth on a micro level. For example, cities in America, despite having lots of economic data on state and county levels, could benefit from understanding more granular information in order to improve policy decisions for infrastructure and public support. \n",
    "\n",
    "In order for this approach to work, we need to be able to extract relevant features from the images in order to train our machine learning model. Our model will not be able to predict the wealth of individual houses (i.e., families), but will work on clusters of houses (i.e., neighborhoods) because of the complexity of wealth measurements and tendency for neighborhood to be at a nearly homogeneous economic level. As a result, we will need to extract \"cluster\" features to process with our (NEURAL NETWORK).\n",
    "\n",
    "Thinking about the kinds of features that would elucidate the wealth of a region, we can start to identify what we need to extract (in some way) from the images. One of the first and most common thoughts is to get satellite imagery of the region at night and observe the night-light intensity; more lights at night tend to correspond with more wealth while less lights at night tend to correspond with poorer areas. Our group has also thought of the following ideas as means to identify wealth:\n",
    "- Number of cars\n",
    "- Percentage of green-space\n",
    "- Number of high-rises\n",
    "- What time traffic occurs at\n",
    "- Housing density\n",
    "- Aerospace/nautical infrastructure\n",
    "\n",
    "The number of of cars tends to be a good indicator or whether a city has passed a certain threshold for wealth. Yes, some cities that are poorer than others will have more cars, but cities that have no cars tend to be the poorest, so we can figure out a baseline level for the wealth of a city if we can extract the number of cars from the image.\n",
    "\n",
    "Percentage of green space is perhaps even less reliable than the number of cars, but can also establish relative rankings of wealth between multiple cities. Cities with lots of public funds, and consequently wealth, will tend to spend money on maintaining public green-spaces. Granted, some rural regions tend to also have a lot of green-space in the form of farms or undeveloped land, so in this case green-space does not correspond to higher wealth. However, if we can ensure that the imagery we are looking at represents a urban city, we could perhaps take into  green-space into account to predict the level of wealth.\n",
    "\n",
    "Number of high-rises is definitely a critical feature of a city's wealth. However, extracting this information from satellite imagery proves to be tricky because of the flatness of the images. One way to get around this is to analyze the shadows produced by buildings at different times of the day. If the buildings are tall, they will cast long shadows at all times of the day (not only briefly in the morning and night).\n",
    "\n",
    "Housing density is highly correlated to the \"urban-ness\" of a region, which in turn is suggestive of the wealth of a city. Rural areas (i.e., poorer, generally) have a lower housing density while urban areas (i.e., wealthier, generally) have a higher housing density. Granted, there are exceptions to this trend, but generally this fact will hold and is one of the easier features to extract from satellite imagery.\n",
    "\n",
    "We will be getting our images from Planet.com, a publicly available database of satellite imagery from the last few years that covers most of the world. Unfortunately, API access is limited to California so we will only be able to run our model using data from California, but there is no reason that this method would not work given more input data from around the world.\n",
    "\n",
    "In this notebook, we'll take you through the entire process from setting up the program to download images and extract features to running the data through the machine learning pipeline and getting a predicted wealth score for input data. \n",
    "\n",
    "First, we'll input the necessary modules. `json` and `io` are just used to load in our Planet.com API key. You can sign up for a free account at https://www.planet.com/. The approval process will take a few days, but after receiving your API key, this entire notebook can be completed in one sitting. We will be using the `requests` module to make API requests for the satellite imagery, which requires authorization using the `requests.auth` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, io, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scipy.ndimage\n",
    "import cv2\n",
    "from PIL import Image, ImageColor\n",
    "from sklearn.svm import SVR, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOADS in your API_KEY from the config_secret.json file\n",
    "with io.open(\"config_secret.json\") as cred:\n",
    "    API_KEY = json.load(cred)[\"API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_URL = \"https://maps.googleapis.com/maps/api/staticmap\"\n",
    "MAX_RGB = 255\n",
    "DEFAULT_SIZE = 600\n",
    "DEFAULT_LABEL_VISIBILITY = \"off\"\n",
    "\n",
    "CAR_EDGE_TEMPLATE = cv2.Canny(cv2.cvtColor(np.asarray(Image.open(\"car_template.png\")), cv2.COLOR_BGR2GRAY), 10, 300)\n",
    "\n",
    "# (lat/pixel, lon/pixel)\n",
    "# Multiply by pixels to see lat/lon span of image\n",
    "# Only want zoom levels 13-20\n",
    "# City zoom = 12\n",
    "ZOOMS = {\n",
    "    12: (0.06/230, 0.06/175),\n",
    "    13: (0.02/153, 0.02/116),\n",
    "    14: (0.008/122, 0.008/122),\n",
    "    15: (0.008/245, 0.008/187),\n",
    "    16: (0.003/183, 0.003/140),\n",
    "    17: (0.002/245, 0.002/186),\n",
    "    18: (0.001/245, 0.001/186),\n",
    "    19: (0.0005/245, 0.0005/186),\n",
    "    20: (0.0002/196, 0.0002/149)\n",
    "}\n",
    "\n",
    "BIG_IMAGE = np.asarray(Image.open(\"night_images/dnb_land_ocean_ice_geo.tif\"))\n",
    "BIGIMAGE_H, BIGIMAGE_W, _ = BIG_IMAGE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_image(content):\n",
    "    return Image.open(io.BytesIO(content)).convert(\"RGBA\").convert(\"RGB\")\n",
    "\n",
    "def rgb_to_hex(rgb_tuple):\n",
    "    return \"0x%02x%02x%02x\" % rgb_tuple\n",
    "\n",
    "def create_payload(mode, (lat, lon), zoom, params={}, ret_colors=True):\n",
    "    size = params.get(\"size\", (DEFAULT_SIZE, DEFAULT_SIZE))\n",
    "    padding = params.get(\"padding\", 0)\n",
    "    road_color = params.get(\"road_color\", (0, MAX_RGB, 0))\n",
    "    road_color_hex = rgb_to_hex(road_color)\n",
    "    man_made_color = params.get(\"man_made_color\", (0, 0, 0))\n",
    "    man_made_color_hex = rgb_to_hex(man_made_color)\n",
    "    poi_color = params.get(\"poi_color\", (MAX_RGB, 0, 0))\n",
    "    poi_color_hex = rgb_to_hex(poi_color)\n",
    "    water_color = params.get(\"water_color\", (0, 0, MAX_RGB))\n",
    "    water_color_hex = rgb_to_hex(water_color)\n",
    "    natural_color = params.get(\"natural_color\", (MAX_RGB, 0, MAX_RGB))\n",
    "    natural_color_hex = rgb_to_hex(natural_color)\n",
    "    label_visibility = params.get(\"label_visibility\", DEFAULT_LABEL_VISIBILITY)\n",
    "\n",
    "    base_payload = [(\"size\", \"{}x{}\".format(size[0],size[1])), (\"key\", API_KEY)]\n",
    "    style_payload = [(\"style\", \"feature:road|element:geometry|color:{}|visibility:on\".format(road_color_hex)),\n",
    "                     (\"style\", \"feature:landscape.man_made|element:geometry.fill|color:{}\".format(man_made_color_hex)),\n",
    "                     (\"style\", \"element:labels|visibility:{}\".format(label_visibility)),\n",
    "                     (\"style\", \"feature:poi|element:geometry|color:{}\".format(poi_color_hex)),\n",
    "                     (\"style\", \"feature:water|element:geometry|color:{}\".format(water_color_hex)),\n",
    "                     (\"style\", \"feature:landscape.natural|element:geometry.fill|color:{}\".format(natural_color_hex))]\n",
    "    satellite_payload = base_payload + [(\"maptype\", \"satellite\")]\n",
    "    road_payload = base_payload + style_payload + [(\"maptype\", \"roadmap\")]\n",
    "    \n",
    "    if mode == \"satellite\": payload = satellite_payload \n",
    "    elif mode == \"road\": payload = road_payload\n",
    "    else: raise ValueError(\"Unrecognized mode '{}'. Mode can either be 'satellite' or 'road'.\".format(mode))\n",
    "        \n",
    "    payload += [(\"zoom\", zoom)] + [(\"center\", \"{},{}\".format(lat, lon))]\n",
    "    colors = {\n",
    "        \"road\": np.array(road_color),\n",
    "        \"man_made\": np.array(man_made_color), \n",
    "        \"poi\": np.array(poi_color),\n",
    "        \"water\": np.array(water_color),\n",
    "        \"natural\": np.array(natural_color)\n",
    "    }\n",
    "    \n",
    "    return (payload, colors) if ret_colors else payload\n",
    "    \n",
    "\n",
    "# bottom left, top right corners\n",
    "def bounding_box((lat1,lon1), (lat2,lon2), zoom):\n",
    "    w = lon2 - lon1\n",
    "    h = lat2 - lat1\n",
    "    \n",
    "    w_per_image = DEFAULT_SIZE * ZOOMS[zoom][1]\n",
    "    h_per_image = DEFAULT_SIZE * ZOOMS[zoom][0]\n",
    "    \n",
    "    num_width = math.ceil(w / w_per_image)\n",
    "    num_height = math.ceil(h / h_per_image)\n",
    "    \n",
    "    lons = np.linspace(lon1 + w_per_image/2, lon2 - w_per_image/2, num=num_width)\n",
    "    lats = np.linspace(lat1 + h_per_image/2, lat2 - h_per_image/2, num=num_height)\n",
    "    \n",
    "    return lats, lons\n",
    "\n",
    "def get_image(lat, lon, payload, zoom):\n",
    "    r = requests.get(BASE_URL, params=payload)\n",
    "    image = load_image(r.content)\n",
    "    return image\n",
    "\n",
    "def get_images((lat1,lon1), (lat2,lon2), zoom, modes, ret_colors=True):\n",
    "    lats, lons = bounding_box((lat1,lon1), (lat2,lon2), zoom)\n",
    "    num_images = len(lats) * len(lons)\n",
    "    images = {\n",
    "        \"road\": [],\n",
    "        \"satellite\": []\n",
    "    }\n",
    "    \n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            for mode in modes:\n",
    "                payload, colors = create_payload(mode, (lat, lon), zoom)\n",
    "                images[mode].append( get_image(lat, lon, payload, zoom) )\n",
    "                \n",
    "    return (images, num_images, colors) if ret_colors else (images, num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# var a = Math.sin(Δφ/2) * Math.sin(Δφ/2) +\n",
    "#        Math.cos(φ1) * Math.cos(φ2) *\n",
    "#        Math.sin(Δλ/2) * Math.sin(Δλ/2);\n",
    "# var c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n",
    "\n",
    "def find_pixel(lat, lon, width, height):\n",
    "    x = int((lon + 180) * (width / 360))\n",
    "    y = int((90 - lat) * (height / 180))\n",
    "    return (x, y)\n",
    "\n",
    "def find_light(x, y):\n",
    "    return BIG_IMAGE[x-10:x+10,y-10:y+10]\n",
    "\n",
    "def average_light(lat, lon):\n",
    "    pixelx, pixely = find_pixel(lat, lon, BIGIMAGE_W, BIGIMAGE_H)\n",
    "    avg_rgb = np.mean(np.mean(find_light(pixelx, pixely), axis=1), axis=0)\n",
    "    # Luminosity formula: 0.2126*R + 0.7152*G + 0.0722*B\n",
    "    return 0.2126 * avg_rgb[0] + 0.7152 * avg_rgb[1] + 0.0722 * avg_rgb[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pixels_of_color(im_arr, color, tolerance=10):\n",
    "    lower_bound = color - tolerance\n",
    "    lower_bound[lower_bound < 0] = 0\n",
    "    upper_bound = color + tolerance\n",
    "    upper_bound[upper_bound > MAX_RGB] = MAX_RGB\n",
    "    return np.all((im_arr >= lower_bound) & (im_arr <= upper_bound), axis=2)\n",
    "\n",
    "def find_roads(road_arr, road_color, tolerance=10):\n",
    "    return get_pixels_of_color(road_arr, road_color, tolerance=tolerance)\n",
    "\n",
    "def road_variance(sat_arr, road_only_arr):\n",
    "    sat_roads = sat_arr[road_only_arr]\n",
    "    return sum(np.std(sat_roads, axis=1))\n",
    "\n",
    "def count_object_pixels(img_rgb, obj_edge_template=CAR_EDGE_TEMPLATE, threshold=0.1, angular_granularity=1):\n",
    "    w, h = obj_edge_template.shape\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "    img_edges = cv2.Canny(img_gray, 100, 200)\n",
    "    loc = (np.array(0), np.array(0))\n",
    "    \n",
    "    for i in range(angular_granularity):\n",
    "        template = scipy.ndimage.rotate(obj_edge_template, i * 360. / angular_granularity, mode=\"constant\")\n",
    "        match_coeff = cv2.matchTemplate(img_edges, template, cv2.TM_CCOEFF_NORMED)\n",
    "        found = np.where(match_coeff > threshold)\n",
    "        loc = (np.append(loc[0], found[0]), np.append(loc[1], found[1]))\n",
    "    \n",
    "    return len(set(zip(*loc)))\n",
    "\n",
    "def extract_features(road_arr, sat_arr, colors, ((lat1, lon1), (lat2, lon2)), tolerance=10):\n",
    "    total_pixels = road_arr.shape[0] * road_arr.shape[1]\n",
    "    road_pixels = {}\n",
    "    road_pixel_counts = {}\n",
    "    \n",
    "    for kind, color in colors.iteritems():\n",
    "        road_pixels[kind] = get_pixels_of_color(road_arr, color, tolerance=tolerance)\n",
    "        \n",
    "    for kind, color_pixels in road_pixels.iteritems():\n",
    "        road_pixel_counts[kind] = np.sum(color_pixels)\n",
    "        \n",
    "    road_var = road_variance(sat_arr, road_pixels[\"road\"])\n",
    "    avg_light = average_light(lat1 + (lat2 - lat1)/2, lon1 + (lon2 - lon1)/2)\n",
    "    car_pixels = float(count_object_pixels(sat_arr)) / road_pixel_counts[\"road\"] if road_pixel_counts[\"road\"] != 0 else 0\n",
    "    color_features = np.array([float(count) / total_pixels for _, count in road_pixel_counts.iteritems()])\n",
    "    other_features = np.array([road_var / 1000, car_pixels, avg_light])\n",
    "\n",
    "    return np.concatenate((color_features, other_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_features_for_city(((lat1, lon1), (lat2, lon2)), zoom=15):\n",
    "    modes = [\"road\", \"satellite\"]\n",
    "    images, n, colors = get_images((lat1, lon1), (lat2, lon2), zoom, modes)\n",
    "    # n pictures, 8 features\n",
    "    features = np.zeros((n, 8))\n",
    "    \n",
    "    for i in xrange(n):\n",
    "        road_arr = np.asarray(images[\"road\"][i])\n",
    "        sat_arr = np.asarray(images[\"satellite\"][i])\n",
    "        features[i] = extract_features(road_arr, sat_arr, colors, ((lat1, lon1), (lat2, lon2)))\n",
    "    \n",
    "    new_features = np.average(features, axis=0)\n",
    "    new_features[0] = np.sum(features[:,0])\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class WealthPredictor:\n",
    "    def __init__(self, zoom=15):\n",
    "        if zoom < 12 or zoom > 20: raise ValueError(\"Zoom must be between 12 and 20 (inclusive)\")\n",
    "        self.zoom = zoom\n",
    "        self.classifier = SVR(kernel=\"linear\")\n",
    "        \n",
    "    def train(self, train_coords, train_labels):\n",
    "        train_features = np.array([get_features_for_city(coord, zoom=self.zoom) for coord in train_coords])\n",
    "        print \"Training features:\", train_features\n",
    "        self.classifier.fit(train_features, train_labels)\n",
    "        print \"Coefficients:\", self.classifier.coef_\n",
    "    \n",
    "    def predict(self, test_coords):\n",
    "        test_features = np.array([get_features_for_city(coord) for coord in test_coords])\n",
    "        print \"Test features:\", test_features\n",
    "        return self.classifier.predict(test_features)\n",
    "    \n",
    "    def score(self, test_coords, test_labels):\n",
    "        test_features = np.array([get_features_for_city(coord) for coord in test_coords])\n",
    "        return self.classifier.score(test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training features: [[ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]]\n",
      "Coefficients: [[  0.00000000e+00   0.00000000e+00  -2.22044605e-15]]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Training...\"\n",
    "\n",
    "training_places = [\"Summerville, GA\", \"San Francisco, CA\", \"DC\", \"Blackwater, AZ\", \n",
    "                   \"Sneedville, TN\", \"Newton, MA\", \"Mamou, LA\",\n",
    "                   \"Kirkland, WA\"]\n",
    "training_coords = [((34.472709, -85.365401), (34.490396, -85.328837)),\n",
    "                   ((37.710978, -122.500087), (37.802606, -122.404110)),\n",
    "                   ((38.819597, -77.160145), (38.983113, -76.912953)),\n",
    "                   ((33.000521, -111.664691), (33.045859, -111.568733)),\n",
    "                   ((36.513680, -83.234828), (36.555887, -83.166506)),\n",
    "                   ((42.288678, -71.267065), (42.366094, -71.160635)), \n",
    "                   ((30.624943, -92.425908), (30.644181, -92.410887)),\n",
    "                   ((47.653627, -122.204879), (47.722267, -122.178787))]\n",
    "training_labels = [17881, 51686, 45477, 7268, 10958, 68122, 14126, 50991]\n",
    "\n",
    "wp = WealthPredictor(14)\n",
    "wp.train(training_coords, training_labels)\n",
    "\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring test coordinates...\n",
      "Test features: [[ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]\n",
      " [ 0.      0.      3.5242]]\n",
      "[ 31679.  31679.  31679.]\n",
      "-0.0215170741987\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Scoring test coordinates...\"\n",
    "\n",
    "training_places = [\"University Park, NM\", \"Pittsburgh, PA\", \"Tenafly, NJ\"]\n",
    "test_coords = [((32.263894, -106.765491), (32.285883, -106.739656)),\n",
    "               ((40.417268, -80.036749), (40.418268, -80.035749)),\n",
    "               ((40.901511, -73.982347), (40.936339, -73.930420))]\n",
    "test_labels = [5520, 28176, 73846]\n",
    "\n",
    "print wp.predict(test_coords)\n",
    "print wp.score(test_coords, test_labels)\n",
    "\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what these satellite images look like, we will show you how to download a single image and then proceed to, what Planet calls, an Area of Interest, or AOI. First, we define a geometry, which is a collection of latitude and longitude points  that forms a polygon around the area you would like to get pictures from. Remember that Planet API only works with California right now, so if you want to change the coordinates, make sure they remain within the state. Our example geometry is centered on a reservoir in Redding, CA. Next, we'll need to define filters for the Planet API; these include the geometry filter discussed above, as well as date range filters (only getting images within a specified date range), cloud cover filters (perhaps you only want to look at images on clear day), and many more. We then send this request to the Stats API endpoint to see how many possible images there are that fit our criteria. In our example, there are 30 images taken of Redding, CA within the date range that have less than 50% cloud cover."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
