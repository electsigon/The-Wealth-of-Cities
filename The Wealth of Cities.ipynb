{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wealth of Cities\n",
    "## Predicting the Wealth of a City from Satellite Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accurate measurements of the economic characteristics of populations critically influence research and policy. Such measurements can shape decisions by governments and how to allocate resources and provide infrastructure to improve human livelihoods in a wide-range of situations. Although economic data is readily available for some developing nations, many regions of the modern world remain unexposed to the benefits of economic analysis where regions lack key measures of economic development and efficiency. Regions such as parts of Africa conduct little to no economic surveys or other means of collecting data on their financial situations. We attempt to address this problem by using publicly available satellite imagery to predict the wealth of a city (or, more generally, a geographic region) based on fundamental features identified in these images and running them through a convolutional neural network. Not only would this method be applicable to regions that lack economic data, but could also be applied to cities with a wealth of economic information on a macro level but a dearth on a micro level. For example, cities in America, despite having lots of economic data on state and county levels, could benefit from understanding more granular information in order to improve policy decisions for infrastructure and public support. \n",
    "\n",
    "In order for this approach to work, we need to be able to extract relevant features from the images in order to train our machine learning model. Our model will not be able to predict the wealth of individual houses (i.e., families), but will work on clusters of houses (i.e., neighborhoods) because of the complexity of wealth measurements and tendency for neighborhood to be at a nearly homogeneous economic level. As a result, we will need to extract \"cluster\" features to process with our (NEURAL NETWORK).\n",
    "\n",
    "Thinking about the kinds of features that would elucidate the wealth of a region, we can start to identify what we need to extract (in some way) from the images. One of the first and most common thoughts is to get satellite imagery of the region at night and observe the night-light intensity; more lights at night tend to correspond with more wealth while less lights at night tend to correspond with poorer areas. Our group has also thought of the following ideas as means to identify wealth:\n",
    "- Number of cars\n",
    "- Percentage of green-space\n",
    "- Number of high-rises\n",
    "- What time traffic occurs at\n",
    "- Housing density\n",
    "- Aerospace/nautical infrastructure\n",
    "\n",
    "The number of of cars tends to be a good indicator or whether a city has passed a certain threshold for wealth. Yes, some cities that are poorer than others will have more cars, but cities that have no cars tend to be the poorest, so we can figure out a baseline level for the wealth of a city if we can extract the number of cars from the image.\n",
    "\n",
    "Percentage of green space is perhaps even less reliable than the number of cars, but can also establish relative rankings of wealth between multiple cities. Cities with lots of public funds, and consequently wealth, will tend to spend money on maintaining public green-spaces. Granted, some rural regions tend to also have a lot of green-space in the form of farms or undeveloped land, so in this case green-space does not correspond to higher wealth. However, if we can ensure that the imagery we are looking at represents a urban city, we could perhaps take into  green-space into account to predict the level of wealth.\n",
    "\n",
    "Number of high-rises is definitely a critical feature of a city's wealth. However, extracting this information from satellite imagery proves to be tricky because of the flatness of the images. One way to get around this is to analyze the shadows produced by buildings at different times of the day. If the buildings are tall, they will cast long shadows at all times of the day (not only briefly in the morning and night).\n",
    "\n",
    "Housing density is highly correlated to the \"urban-ness\" of a region, which in turn is suggestive of the wealth of a city. Rural areas (i.e., poorer, generally) have a lower housing density while urban areas (i.e., wealthier, generally) have a higher housing density. Granted, there are exceptions to this trend, but generally this fact will hold and is one of the easier features to extract from satellite imagery.\n",
    "\n",
    "We will be getting our images from Planet.com, a publicly available database of satellite imagery from the last few years that covers most of the world. Unfortunately, API access is limited to California so we will only be able to run our model using data from California, but there is no reason that this method would not work given more input data from around the world.\n",
    "\n",
    "In this notebook, we'll take you through the entire process from setting up the program to download images and extract features to running the data through the machine learning pipeline and getting a predicted wealth score for input data. \n",
    "\n",
    "First, we'll input the necessary modules. `json` and `io` are just used to load in our Planet.com API key. You can sign up for a free account at https://www.planet.com/. The approval process will take a few days, but after receiving your API key, this entire notebook can be completed in one sitting. We will be using the `requests` module to make API requests for the satellite imagery, which requires authorization using the `requests.auth` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, io, math\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "from scipy.misc import toimage\n",
    "import scipy.ndimage\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOADS in your API_KEY from the config_secret.json file\n",
    "with io.open(\"config_secret.json\") as cred:\n",
    "    API_KEY = json.load(cred)[\"API_KEY\"]\n",
    "\n",
    "BASE_URL = \"https://maps.googleapis.com/maps/api/staticmap\"\n",
    "MODES = [\"road\", \"satellite\"]\n",
    "MAX_RGB = 255\n",
    "DEFAULT_SIZE = 600\n",
    "DEFAULT_VISIBILITY = \"off\"\n",
    "\n",
    "CAR_EDGE_TEMPLATE = cv2.Canny(cv2.cvtColor(np.asarray(Image.open(\"car_template.png\")), cv2.COLOR_BGR2GRAY), 10, 300)\n",
    "\n",
    "# (lat/pixel, lon/pixel)\n",
    "# Multiply by pixels to see lat/lon span of image\n",
    "# Only want zoom levels 13-20\n",
    "# City zoom = 12\n",
    "ZOOMS = {\n",
    "    12: (0.06/230, 0.06/175),\n",
    "    13: (0.02/153, 0.02/116),\n",
    "    14: (0.008/122, 0.008/122),\n",
    "    15: (0.008/245, 0.008/187),\n",
    "    16: (0.003/183, 0.003/140),\n",
    "    17: (0.002/245, 0.002/186),\n",
    "    18: (0.001/245, 0.001/186),\n",
    "    19: (0.0005/245, 0.0005/186),\n",
    "    20: (0.0002/196, 0.0002/149)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EARTH_RADIUS = 6371000\n",
    "BIGIMAGE_W, BIGIMAGE_H = (54000, 27000)\n",
    "OCTANTIMAGE_W, OCTANTIMAGE_H = (13500, 13500)\n",
    "BIG_IMAGE = np.asarray(Image.open(\"night_images/dnb_land_ocean_ice_geo.tif\"))\n",
    "OCTANT_IMAGES = [np.asarray(Image.open(\"night_images/dnb_land_ocean_ice.{}_geo.tif\".format(i+1))) for i in xrange(8)]\n",
    "OCTANT_RATIOS = [(0, 0), (BIGIMAGE_W/4.0, 0), (BIGIMAGE_W/2.0, 0), (3*BIGIMAGE_W/4.0, 0),\n",
    "                 (0, BIGIMAGE_H/2.0), (BIGIMAGE_W/4.0, BIGIMAGE_H/2.0), (BIGIMAGE_W/2.0, BIGIMAGE_H/2.0), \n",
    "                 (3*BIGIMAGE_W/4.0, BIGIMAGE_H/2.0)]\n",
    "OCTANTS = zip(OCTANT_IMAGES, OCTANT_RATIOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_image(content):\n",
    "    return Image.open(io.BytesIO(content)).convert(\"RGBA\").convert(\"RGB\")\n",
    "\n",
    "def rgb_to_hex(rgb_tuple):\n",
    "    return \"0x%02x%02x%02x\" % rgb_tuple\n",
    "\n",
    "def create_payload(mode, (lat, lon), zoom, params={}, ret_colors=True):\n",
    "\n",
    "    size = params.get(\"size\", (DEFAULT_SIZE, DEFAULT_SIZE))\n",
    "    padding = params.get(\"padding\", 0)\n",
    "    road_color = params.get(\"road_color\", (0, MAX_RGB, 0))\n",
    "    road_color_hex = rgb_to_hex(road_color)\n",
    "    man_made_color = params.get(\"man_made_color\", (0, 0, 0))\n",
    "    man_made_color_hex = rgb_to_hex(man_made_color)\n",
    "    poi_color = params.get(\"poi_color\", (MAX_RGB, 0, 0))\n",
    "    poi_color_hex = rgb_to_hex(poi_color)\n",
    "    water_color = params.get(\"water_color\", (0, 0, MAX_RGB))\n",
    "    water_color_hex = rgb_to_hex(water_color)\n",
    "    natural_color = params.get(\"natural_color\", (MAX_RGB, 0, MAX_RGB))\n",
    "    natural_color_hex = rgb_to_hex(natural_color)\n",
    "    label_visibility = params.get(\"label_visibility\", DEFAULT_VISIBILITY)\n",
    "\n",
    "    base_payload = [(\"size\", \"{}x{}\".format(size[0],size[1])), (\"key\", API_KEY)]\n",
    "    style_payload = [(\"style\", \"feature:road|element:geometry|color:{}|visibility:on\".format(road_color_hex)),\n",
    "                     (\"style\", \"feature:landscape.man_made|element:geometry.fill|color:{}\".format(man_made_color_hex)),\n",
    "                     (\"style\", \"element:labels|visibility:{}\".format(label_visibility)),\n",
    "                     (\"style\", \"feature:poi|element:geometry|color:{}\".format(poi_color_hex)),\n",
    "                     (\"style\", \"feature:water|element:geometry|color:{}\".format(water_color_hex)),\n",
    "                     (\"style\", \"feature:landscape.natural|element:geometry.fill|color:{}\".format(natural_color_hex))]\n",
    "    satellite_payload = base_payload + [(\"maptype\", \"satellite\")]\n",
    "    road_payload = base_payload + style_payload + [(\"maptype\", \"roadmap\")]\n",
    "    \n",
    "    if mode == \"satellite\": payload = satellite_payload \n",
    "    elif mode == \"road\": payload = road_payload\n",
    "    else: raise ValueError(\"Unrecognized mode '{}'. Mode can either be 'satellite' or 'road'.\".format(mode))\n",
    "        \n",
    "    payload += [(\"zoom\", zoom)] + [(\"center\", \"{},{}\".format(lat, lon))]\n",
    "    colors = {\n",
    "        \"road\": np.array(road_color),\n",
    "        \"man_made\": np.array(man_made_color), \n",
    "        \"poi\": np.array(poi_color),\n",
    "        \"water\": np.array(water_color),\n",
    "        \"natural\": np.array(natural_color)\n",
    "    }\n",
    "    \n",
    "    return (payload, colors) if ret_colors else payload\n",
    "    \n",
    "\n",
    "# bottom left, top right corners\n",
    "def bounding_box((lat1,lon1), (lat2,lon2), zoom):\n",
    "    w = lon2 - lon1\n",
    "    h = lat2 - lat1\n",
    "    \n",
    "    w_per_image = DEFAULT_SIZE * ZOOMS[zoom][1]\n",
    "    h_per_image = DEFAULT_SIZE * ZOOMS[zoom][0]\n",
    "    \n",
    "    num_width = math.ceil(w / w_per_image)\n",
    "    num_height = math.ceil(h / h_per_image)\n",
    "    \n",
    "    lons = np.linspace(lon1 + w_per_image/2, lon2 - w_per_image/2, num=num_width)\n",
    "    lats = np.linspace(lat1 + h_per_image/2, lat2 - h_per_image/2, num=num_height)\n",
    "    \n",
    "    return lats, lons\n",
    "\n",
    "def get_image(lat, lon, payload, zoom):\n",
    "    r = requests.get(BASE_URL, params=payload)\n",
    "    image = load_image(r.content)\n",
    "    return image\n",
    "\n",
    "def get_images((lat1,lon1), (lat2,lon2), zoom, modes, ret_colors=True):\n",
    "    lats, lons = bounding_box((lat1,lon1), (lat2,lon2), zoom)\n",
    "    num_images = len(lats) * len(lons)\n",
    "    print \"Downloading {} images...\".format(num_images)\n",
    "    images = {\n",
    "        \"road\": [],\n",
    "        \"satellite\": []\n",
    "    }\n",
    "    \n",
    "    i = 1\n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            for mode in modes:\n",
    "                print \"\\tDownloading image \" + i\n",
    "                i += 1\n",
    "                payload, colors = create_payload(mode, (lat, lon), zoom)\n",
    "                images[mode].append( get_image(lat, lon, payload, zoom) )\n",
    "                \n",
    "    print \"Done downloading images!\"        \n",
    "    return (images, num_images, colors) if ret_colors else (images, num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_loc(lat, lon, radius):\n",
    "    y = np.sqrt(2*radius**2 * (1 - np.cos(lat)))\n",
    "    x = np.sqrt(2*radius**2 * (1 - np.cos(lon)))\n",
    "    return x, y\n",
    "\n",
    "def find_pixel(lat, lon, radius, scale):\n",
    "    x, y = find_loc(lat, lon, radius)\n",
    "    centerx = BIGIMAGE_W / 2\n",
    "    centery = BIGIMAGE_H / 2\n",
    "    return centerx + x/scale, centery + y/scale\n",
    "\n",
    "def find_octant(latR, lonR):\n",
    "    if (lonR < 0.5):\n",
    "        if (latR < 0.25):\n",
    "            return 0\n",
    "        elif (latR < 0.5):\n",
    "            return 1\n",
    "        elif (latR < 0.75):\n",
    "            return 2\n",
    "        else: \n",
    "            return 3\n",
    "    else:\n",
    "        if (latR < 0.25):\n",
    "            return 4\n",
    "        elif (latR < 0.5):\n",
    "            return 5\n",
    "        elif (latR < 0.75):\n",
    "            return 6\n",
    "        else:\n",
    "            return 7\n",
    "\n",
    "def find_light(x, y, bigimage, octants):\n",
    "    octant_num = find_octant(x / BIGIMAGE_W, y / BIGIMAGE_H)\n",
    "    octantimage, (ox, oy) = octants[octant_num]\n",
    "    x2 = OCTANTIMAGE_W * (x-ox) / (BIGIMAGE_W/4.0) \n",
    "    y2 = OCTANTIMAGE_H * (y-oy) / (BIGIMAGE_H/2.0)\n",
    "    return octantimage[x2][y2]\n",
    "\n",
    "def average_light(lat, lon):\n",
    "    pixelx, pixely = find_pixel(lat, lon, EARTH_RADIUS, 750)\n",
    "    pixels = [(pixelx, pixely), (pixelx + 1, pixely),(pixelx - 1, pixely), (pixelx, pixely - 1), (pixelx, pixely + 1)]\n",
    "    return np.mean([find_light(x, y, BIG_IMAGE, OCTANTS) for (x, y) in pixels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pixels_of_color(im_arr, color, tolerance=10):\n",
    "    lower_bound = color - tolerance\n",
    "    lower_bound[lower_bound < 0] = 0\n",
    "    upper_bound = color + tolerance\n",
    "    upper_bound[upper_bound > MAX_RGB] = MAX_RGB\n",
    "    return np.all((im_arr >= lower_bound) & (im_arr <= upper_bound), axis=2)\n",
    "\n",
    "def find_roads(road_arr, road_color, tolerance=10):\n",
    "    return get_pixels_of_color(road_arr, road_color, tolerance=tolerance)\n",
    "\n",
    "def road_variance(sat_arr, road_only_arr):\n",
    "    sat_roads = sat_arr[road_only_arr]\n",
    "    return sum(np.std(sat_roads, axis=1))\n",
    "\n",
    "def count_object_pixels(img_rgb, obj_edge_template=CAR_EDGE_TEMPLATE, threshold=0.1, angular_granularity=1):\n",
    "    img_rgb = np.asarray(img_rgb)\n",
    "    w, h = obj_edge_template.shape\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "    img_edges = cv2.Canny(img_gray, 100, 200)\n",
    "    loc = (np.array(0), np.array(0))\n",
    "    \n",
    "    for i in range(angular_granularity):\n",
    "        template = scipy.ndimage.rotate(obj_edge_template, i * 360. / angular_granularity, mode=\"constant\")\n",
    "        match_coeff = cv2.matchTemplate(img_edges, template, cv2.TM_CCOEFF_NORMED)\n",
    "        found = np.where(match_coeff > threshold)\n",
    "        loc = (np.append(loc[0], found[0]), np.append(loc[1], found[1]))\n",
    "    \n",
    "    return len(set(zip(*loc)))\n",
    "\n",
    "def extract_features(road_arr, sat_arr, colors, ((lat1, lon1), (lat2, lon2)), tolerance=10):\n",
    "    total_pixels = road_arr.shape[0] * road_arr.shape[1]\n",
    "    road_pixels = {}\n",
    "    road_pixel_counts = {}\n",
    "    \n",
    "    for kind, color in colors.iteritems():\n",
    "        road_pixels[kind] = get_pixels_of_color(road_arr, color, tolerance=tolerance)\n",
    "        \n",
    "    for kind, color_pixels in road_pixels.iteritems():\n",
    "        road_pixel_counts[kind] = np.sum(color_pixels)\n",
    "        \n",
    "    road_var = road_variance(sat_arr, road_pixels[\"road\"])\n",
    "    \n",
    "    avg_light = average_light(lat1 + (lat2 - lat1)/2, lon1 + (lon2 - lon1)/2)\n",
    "    \n",
    "    feature1 = np.array(road_pixels.items())\n",
    "    feature2 = np.array(road_pixel_counts.items()) / total_pixels\n",
    "    feature3 = np.array([road_var])\n",
    "    feature4 = np.array([avg_light])\n",
    "    print feature1\n",
    "    print feature2\n",
    "    print feature3\n",
    "    print feature4\n",
    "    print np.concatenate(feature1, feature2, feature3, feature4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'MODES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-51937e19d3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m           ((36.513680, -83.234828), (36.555887, -83.166506))]\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mget_features_for_city\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOINTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# FEATURES = [get_features_for_city(city) for city in points]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-51937e19d3ce>\u001b[0m in \u001b[0;36mget_features_for_city\u001b[0;34m(((lat1, lon1), (lat2, lon2)))\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_features_for_city\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlat2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlat2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'MODES' is not defined"
     ]
    }
   ],
   "source": [
    "def get_features_for_city(((lat1, lon1), (lat2, lon2))):\n",
    "    zoom = 13\n",
    "    modes = MODES\n",
    "    images, n, colors = get_images((lat1, lon1), (lat2, lon2), 12, modes)\n",
    "    \n",
    "#     for i in xrange(1):\n",
    "#         road_arr = np.asarray(images[\"road\"][i])\n",
    "#         sat_arr = np.asarray(images[\"satellite\"][i])\n",
    "#         extract_features(road_arr, sat_arr, colors, ((lat1, lon1), (lat2, lon2)))\n",
    "\n",
    "NAMES = [\"Pittsburgh, PA\", \"San Fransisco, CA\", \"New York, NY\", \"DC\", \"Blackwater, AZ\", \"Sneedsville, TN\"]\n",
    "POINTS = [((40.417268, -80.036749), (40.418268, -80.035749)),\n",
    "          ((37.710978, -122.500087), (37.802606, -122.404110)),\n",
    "          ((40.510705, -74.252698), (40.867683, -73.764113)),\n",
    "          ((38.819597, -77.160145), (38.983113, -76.912953)),\n",
    "          ((33.000521, -111.664691), (33.045859, -111.568733)),\n",
    "          ((36.513680, -83.234828), (36.555887, -83.166506))]\n",
    "\n",
    "get_features_for_city(POINTS[0])\n",
    "\n",
    "# FEATURES = [get_features_for_city(city) for city in points]\n",
    "LABELS = [63024, 59799, 57291, 9491, 13719]\n",
    "# clf = svm.SVR()\n",
    "# clf.fit(features, labels)\n",
    "# clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TESTS ###\n",
    "# zoom = 13 -> 6 images (1.5 seconds)\n",
    "# PGH = ((40.417268, -80.036749), (40.503523, -79.823013), 13, MODES)\n",
    "# zoom = 20 -> 4 images (1 second)\n",
    "# pgh_images, n, colors = get_images((40.417268, -80.036749), (40.418268, -80.035749), 20, MODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what these satellite images look like, we will show you how to download a single image and then proceed to, what Planet calls, an Area of Interest, or AOI. First, we define a geometry, which is a collection of latitude and longitude points  that forms a polygon around the area you would like to get pictures from. Remember that Planet API only works with California right now, so if you want to change the coordinates, make sure they remain within the state. Our example geometry is centered on a reservoir in Redding, CA. Next, we'll need to define filters for the Planet API; these include the geometry filter discussed above, as well as date range filters (only getting images within a specified date range), cloud cover filters (perhaps you only want to look at images on clear day), and many more. We then send this request to the Stats API endpoint to see how many possible images there are that fit our criteria. In our example, there are 30 images taken of Redding, CA within the date range that have less than 50% cloud cover."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
